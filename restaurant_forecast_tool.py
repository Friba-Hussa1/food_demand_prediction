#!/usr/bin/env python3
"""
Restaurant Inventory Forecasting Tool
=====================================

A simple tool for restaurant managers to get next week's inventory recommendations.
This tool loads the trained model and generates actionable forecasts.

Usage: python restaurant_forecast_tool.py [--days 7]
"""

import pandas as pd
import numpy as np
import joblib
import argparse
from datetime import datetime, timedelta
import os

# Optional ARIMA comparison functionality
try:
    import arima_forecasting
    ARIMA_AVAILABLE = True
    
    def run_arima_pipeline(dataset_path=None):
        return arima_forecasting.main(dataset_path)
except ImportError:
    ARIMA_AVAILABLE = False
    
    def run_arima_pipeline(dataset_path=None):
        return None

# Optional autoencoder anomaly detection functionality
AUTOENCODER_AVAILABLE = False
autoencoder_anomaly_detection = None

def run_anomaly_detection(dataset_path, threshold_type='balanced'):
    if AUTOENCODER_AVAILABLE:
        return autoencoder_anomaly_detection.detect_anomalies(dataset_path, threshold_type)
    else:
        print("‚ö†Ô∏è  Autoencoder functionality not available due to TensorFlow/NumPy compatibility issues.")
        print("   To fix: downgrade NumPy with 'uv add numpy<2' or upgrade TensorFlow")
        return None

def train_anomaly_detector(dataset_path, n_trials=50):
    if AUTOENCODER_AVAILABLE:
        return autoencoder_anomaly_detection.train_autoencoder(dataset_path, n_trials)
    else:
        print("‚ö†Ô∏è  Autoencoder functionality not available due to TensorFlow/NumPy compatibility issues.")
        print("   To fix: downgrade NumPy with 'uv add numpy<2' or upgrade TensorFlow")
        return None

def check_anomaly_model_exists():
    if AUTOENCODER_AVAILABLE:
        required_files = [
            'Autoencoder/inventory_autoencoder_model.h5',
            'Autoencoder/inventory_scaler.pkl',
            'Autoencoder/anomaly_threshold.json',
            'Autoencoder/feature_columns.json'
        ]
        return all(os.path.exists(file) for file in required_files)
    return False

# Try to import autoencoder functionality
try:
    from Autoencoder import autoencoder_anomaly_detection
    AUTOENCODER_AVAILABLE = True
except ImportError as e:
    AUTOENCODER_AVAILABLE = False
    print(f"‚ö†Ô∏è  Autoencoder functionality disabled due to import error: {str(e)[:100]}...")

def load_regression_models():
    """Load all trained regression models and preprocessing objects"""
    try:
        # Load all regression models
        models = {}
        model_files = [
            'lasso_model.pkl',
            'linear_regression_model.pkl', 
            'ridge_model.pkl',
            'elasticnet_model.pkl'
        ]
        
        for model_file in model_files:
            try:
                model_name = model_file.replace('_model.pkl', '').replace('_', ' ').title()
                models[model_name] = joblib.load(f'models/regression/{model_file}')
            except FileNotFoundError:
                continue
        
        scaler = joblib.load('models/regression/scaler.pkl')
        selector_info = joblib.load('models/regression/feature_selector_info.pkl')
        
        print(f"‚úÖ Loaded {len(models)} regression models successfully!")
        return models, scaler, selector_info
    except FileNotFoundError as e:
        print("‚ùå Error: Regression models not found!")
        print("Please run 'uv run inventory_forecasting_regression.py' first to train the models.")
        return None, None, None

def load_arima_models():
    """Load all trained ARIMA models"""
    try:
        arima_models = {}
        arima_performance = joblib.load('models/arima/arima_performance.pkl')
        model_metadata = joblib.load('models/arima/arima_metadata.pkl')
        
        # Get target columns from the saved performance data
        target_cols = list(arima_performance.keys())
        
        for col in target_cols:
            try:
                arima_models[col] = joblib.load(f'models/arima/arima_{col}_model.pkl')
            except FileNotFoundError:
                continue
        
        print(f"‚úÖ Loaded {len(arima_models)} ARIMA models successfully!")
        return arima_models, arima_performance, model_metadata, target_cols
    except FileNotFoundError as e:
        print("‚ùå Error: ARIMA models not found!")
        print("Please run 'uv run arima_forecasting.py' first to train the ARIMA models.")
        return None, None, None, None

def get_recent_data(dataset_path):
    """Load recent historical data for feature generation"""
    try:
        df = pd.read_csv(dataset_path)
        df = df.sort_values("delivery_date").reset_index(drop=True)
        df["delivery_date"] = pd.to_datetime(df["delivery_date"])
        
        # Get last 30 days for feature calculation
        recent_df = df.tail(30).copy()
        print(f"‚úÖ Loaded recent data: {len(recent_df)} records from {dataset_path}")
        return recent_df
    except FileNotFoundError:
        print(f"‚ùå Error: Historical data not found at {dataset_path}!")
        print("Please ensure the dataset file exists and the path is correct.")
        return None

def generate_forecast_features(recent_df, forecast_days=7):
    """Generate comprehensive features matching the training feature engineering exactly"""
    # Import the feature engineering function from the training module
    from inventory_forecasting_regression import feature_engineering
    
    # Get target columns dynamically from the data
    exclude_cols = ['delivery_date', 'day_of_week', 'month', 'day_of_month', 'is_weekend', 'days_since_start']
    target_cols = [col for col in recent_df.columns if col not in exclude_cols and not any(suffix in col for suffix in ['_lag', '_roll', '_ratio', '_total'])]
    
    # Create a comprehensive feature-engineered dataset that matches training
    # Start with recent data and extend it with forecast rows
    df_extended = recent_df.copy()
    df_extended['delivery_date'] = pd.to_datetime(df_extended['delivery_date'])
    
    # Get the last date
    last_date = df_extended['delivery_date'].max()
    
    # Generate future rows with estimated values
    for day in range(1, forecast_days + 1):
        future_date = last_date + timedelta(days=day)
        
        # Create a new row with estimated target values based on recent patterns
        new_row = {}
        new_row['delivery_date'] = future_date
        
        # Estimate target values using day-of-week patterns and recent trends
        dow = future_date.weekday()
        day_factors = {0: 0.95, 1: 0.98, 2: 1.02, 3: 1.05, 4: 1.15, 5: 1.25, 6: 1.20}  # Mon-Sun
        day_factor = day_factors.get(dow, 1.0)
        
        for col in target_cols:
            # Get same day of week historical data
            same_dow_data = df_extended[df_extended['delivery_date'].dt.dayofweek == dow]
            if len(same_dow_data) >= 2:
                base_value = same_dow_data[col].tail(4).mean()
            else:
                base_value = df_extended[col].tail(7).mean()
            
            # Apply day factor and add some trend
            recent_trend = df_extended[col].tail(7).pct_change().mean()
            trend_factor = 1.0 + (recent_trend * 0.1) if not np.isnan(recent_trend) else 1.0
            
            new_row[col] = base_value * day_factor * trend_factor
        
        # Add the new row to the extended dataframe
        df_extended = pd.concat([df_extended, pd.DataFrame([new_row])], ignore_index=True)
    
    # Apply the EXACT SAME feature engineering as in training
    df_fe = feature_engineering(df_extended)
    
    # Return only the forecast rows (last forecast_days rows) with all features
    forecast_features = df_fe.tail(forecast_days).copy()
    forecast_features['forecast_date'] = forecast_features['delivery_date']
    
    # Fill any remaining NaN values with forward fill
    forecast_features = forecast_features.fillna(method='ffill').fillna(method='bfill')
    
    return forecast_features

def make_regression_predictions(models, scaler, selector_info, forecast_features, target_cols, best_model_name=None, model_info=None):
    """Make predictions using regression models"""
    
    # Run composite scoring only if we don't have a model from training
    if best_model_name is None:
        try:
            print(f"üîç Running composite model evaluation...")
            composite_best_model, _ = get_best_regression_model_info()
            best_model_name = composite_best_model
            print(f"üéØ Selected from composite scoring: {best_model_name}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not run composite model evaluation: {str(e)}")
            # Fallback: use the first available model
            best_model_name = list(models.keys())[0]
            print(f"üéØ Using fallback model: {best_model_name}")
    else:
        print(f"üéØ Using model from training: {best_model_name}")
    
    # Store model info for later use in accuracy reporting
    if model_info:
        forecast_features._model_info = model_info
    
    # Use best model or fallback to available model
    if best_model_name in models:
        model = models[best_model_name]
    else:
        # Find the best available model from loaded models
        available_models = list(models.keys())
        print(f"‚ö†Ô∏è  Best model '{best_model_name}' not found in loaded models: {available_models}")
        
        # Try to match partial names (e.g., "Random Forest" matches "Random Forest")
        model_found = False
        for model_name in available_models:
            if best_model_name.lower() in model_name.lower() or model_name.lower() in best_model_name.lower():
                model = models[model_name]
                best_model_name = model_name
                print(f"‚úÖ Found matching model: {best_model_name}")
                model_found = True
                break
        
        if not model_found:
            # Use first available model as fallback
            model = list(models.values())[0]
            best_model_name = available_models[0]
            print(f"‚ö†Ô∏è  Using {best_model_name} model as fallback")
    
    # Prepare features (exclude date columns and target columns)
    target_cols_set = set(target_cols)
    feature_cols = [col for col in forecast_features.columns 
                   if col not in ['forecast_date', 'delivery_date'] and col not in target_cols_set]
    
    # Ensure we have the same features as training
    X = forecast_features[feature_cols].values
    
    # Scale features
    X_scaled = scaler.transform(X)
    
    # Select features using the saved indices
    X_selected = X_scaled[:, selector_info['indices']]
    
    # Make predictions
    predictions = model.predict(X_selected)
    
    # Ensure non-negative predictions
    predictions = np.maximum(predictions, 0)
    
    # Create results dataframe
    results = pd.DataFrame()
    results['Date'] = forecast_features['forecast_date'].dt.strftime('%Y-%m-%d')
    results['Day_of_Week'] = forecast_features['forecast_date'].dt.day_name()
    results['Is_Weekend'] = forecast_features['is_weekend'].astype(bool)
    results['Model_Type'] = 'Regression'
    results['Model_Name'] = best_model_name
    
    # Store model info for accuracy reporting if available
    if model_info and 'best_model_metrics' in model_info:
        metrics = model_info['best_model_metrics']
        results._model_accuracy = f"~{metrics['R2']*100:.0f}% (¬±{metrics['MAE']:.0f} units average error)"
    
    # Add predictions (rounded to integers, ensure non-negative with minimum thresholds)
    for i, col in enumerate(target_cols):
        # Handle array of predictions properly - apply max to each element
        forecast_vals = np.maximum(0, np.round(predictions[:, i])).astype(int)
        stock_vals = np.maximum(0, np.round(predictions[:, i] * 1.2)).astype(int)
        
        # Apply minimum reasonable values to prevent zero forecasts
        if col in ['wings', 'tenders']:
            forecast_vals = np.maximum(forecast_vals, 50)  # Minimum 50 units for main items
            stock_vals = np.maximum(stock_vals, 60)
        elif col in ['dips', 'flavours']:
            forecast_vals = np.maximum(forecast_vals, 100)  # Minimum 100 units for condiments
            stock_vals = np.maximum(stock_vals, 120)
        elif col in ['drinks']:
            forecast_vals = np.maximum(forecast_vals, 50)   # Minimum 50 units for drinks
            stock_vals = np.maximum(stock_vals, 60)
        else:  # fries_reg, fries_large, veggies
            forecast_vals = np.maximum(forecast_vals, 20)   # Minimum 20 units for sides
            stock_vals = np.maximum(stock_vals, 24)
        
        results[f'{col.title()}_Forecast'] = forecast_vals
        results[f'{col.title()}_Recommended_Stock'] = stock_vals
    
    return results

def make_arima_predictions(arima_models, model_metadata, forecast_features, target_cols):
    """Make predictions using ARIMA models"""
    
    # Create results dataframe
    results = pd.DataFrame()
    results['Date'] = forecast_features['forecast_date'].dt.strftime('%Y-%m-%d')
    results['Day_of_Week'] = forecast_features['forecast_date'].dt.day_name()
    results['Is_Weekend'] = forecast_features['is_weekend'].astype(bool)
    results['Model_Type'] = 'ARIMA'
    results['Model_Name'] = 'ARIMA Ensemble'
    
    # Prepare external regressors - match what was used in training
    exog_cols = ['day_of_week', 'is_weekend', 'is_friday', 'is_monday', 'month', 
                 'month_sin', 'month_cos', 'dow_sin', 'dow_cos', 'is_month_start']
    
    # Create the external regressors for forecasting
    future_exog = pd.DataFrame()
    future_dates = pd.to_datetime(forecast_features['forecast_date'])
    
    future_exog['day_of_week'] = future_dates.dt.dayofweek
    future_exog['is_weekend'] = (future_dates.dt.dayofweek >= 5).astype(int)
    future_exog['is_friday'] = (future_dates.dt.dayofweek == 4).astype(int)
    future_exog['is_monday'] = (future_dates.dt.dayofweek == 0).astype(int)
    future_exog['month'] = future_dates.dt.month
    future_exog['month_sin'] = np.sin(2 * np.pi * future_dates.dt.month / 12)
    future_exog['month_cos'] = np.cos(2 * np.pi * future_dates.dt.month / 12)
    future_exog['dow_sin'] = np.sin(2 * np.pi * future_dates.dt.dayofweek / 7)
    future_exog['dow_cos'] = np.cos(2 * np.pi * future_dates.dt.dayofweek / 7)
    future_exog['is_month_start'] = (future_dates.dt.day <= 5).astype(int)
    
    forecast_days = len(forecast_features)
    
    # Generate forecasts for each item
    for col in target_cols:
        if col in arima_models:
            try:
                model = arima_models[col]
                metadata = model_metadata.get(col, {})
                model_type = metadata.get('model_type', 'ARIMA')
                log_transformed = metadata.get('log_transformed', False)
                
                if model_type == 'ExpSmoothing':
                    # Exponential smoothing doesn't use exog
                    forecast = model.forecast(steps=forecast_days)
                else:
                    # ARIMA/SARIMA models use exog
                    forecast = model.forecast(steps=forecast_days, exog=future_exog)
                
                # Convert to numpy array if it's a pandas Series
                if hasattr(forecast, 'values'):
                    forecast = forecast.values
                
                # Transform back if log transformation was applied
                if log_transformed:
                    forecast = np.expm1(forecast)
                
                # Handle NaN, inf, and negative values - ensure all forecasts are positive
                forecast = np.nan_to_num(forecast, nan=50.0, posinf=1000.0, neginf=0.0)
                forecast = np.maximum(forecast, 1.0)  # Minimum 1 unit
                
                # Apply reasonable bounds based on item type
                if col in ['wings', 'tenders']:
                    forecast = np.clip(forecast, 50, 10000)  # Main items
                elif col in ['dips', 'flavours']:
                    forecast = np.clip(forecast, 100, 2000)  # Condiments
                else:
                    forecast = np.clip(forecast, 10, 1000)   # Sides
                
                results[f'{col.title()}_Forecast'] = np.round(forecast).astype(int)
                # Add safety stock (20% buffer)
                results[f'{col.title()}_Recommended_Stock'] = np.round(forecast * 1.2).astype(int)
                
            except Exception as e:
                print(f"‚ö†Ô∏è ARIMA forecast failed for {col}, using reasonable defaults: {str(e)}")
                # Fallback to reasonable defaults based on item type
                if col in ['wings', 'tenders']:
                    default_val = 500  # Main items
                elif col in ['dips', 'flavours']:
                    default_val = 300  # Condiments
                else:
                    default_val = 150  # Sides
                
                forecast_vals = [int(default_val)] * forecast_days
                results[f'{col.title()}_Forecast'] = forecast_vals
                results[f'{col.title()}_Recommended_Stock'] = [int(v * 1.2) for v in forecast_vals]
        else:
            # If ARIMA model not available for this item, use reasonable defaults
            if col in ['wings', 'tenders']:
                default_val = 500  # Main items
            elif col in ['dips', 'flavours']:
                default_val = 300  # Condiments
            else:
                default_val = 150  # Sides
            
            forecast_vals = [int(default_val)] * forecast_days
            results[f'{col.title()}_Forecast'] = forecast_vals
            results[f'{col.title()}_Recommended_Stock'] = [int(v * 1.2) for v in forecast_vals]
    
    return results

def calculate_portfolio_performance(actual_dict, pred_dict, target_cols):
    """
    Calculate portfolio-level performance metrics for fair model comparison.
    
    This function implements a sophisticated comparison methodology that addresses the fundamental
    challenge of comparing holistic (one model for all items) vs per-item (separate models) approaches.
    
    Why Portfolio MAPE is Calculated This Way:
    ==========================================
    
    Traditional Approach (WRONG):
    - Calculate MAPE for each item individually
    - Average all item MAPEs equally
    - Problem: Small items (veggies ~150 units) get same weight as large items (wings ~6000 units)
    
    Portfolio Approach (CORRECT):
    - Sum all errors across all items
    - Sum all actual values across all items  
    - Calculate percentage: (total_error / total_actual) √ó 100
    - Result: Natural weighting by business volume and impact
    
    Mathematical Justification:
    ==========================
    
    This formula ensures:
    1. High-volume items (wings: 6000 units) naturally get more influence than low-volume (veggies: 150)
    2. Business impact is properly weighted - 10% error on wings (600 units) vs 10% on veggies (15 units)
    3. Fair comparison between holistic and per-item approaches on same scale
    4. Direct translation to inventory cost impact
    
    Why This Matters for Model Comparison:
    =====================================
    
    Holistic Regression:
    - One model predicts all 8 items simultaneously
    - Captures cross-item relationships (wings‚Üîdips correlation)
    - Portfolio MAPE reflects total inventory accuracy
    
    Per-Item ARIMA:
    - 8 separate specialized models
    - No cross-item learning but item-specific optimization
    - Portfolio MAPE aggregates all individual predictions fairly
    
    Business Relevance:
    ==================
    Restaurant managers care about:
    - Total inventory accuracy (not individual item averages)
    - Cost-weighted performance (high-volume items matter more)
    - Operational efficiency (one model vs eight models)
    
    The portfolio approach revealed regression's 4.9% vs ARIMA's 9.0% represents
    46.1% better total inventory accuracy, directly translating to reduced waste/stockouts.
    """
    
    # Business-weighted importance (adjust based on your restaurant's priorities)
    item_weights = {
        'wings': 0.25, 'tenders': 0.25,      # Main items - highest impact
        'drinks': 0.15, 'fries_reg': 0.10,   # Popular sides/drinks
        'dips': 0.08, 'flavours': 0.08,      # Condiments - moderate impact
        'fries_large': 0.05, 'veggies': 0.04  # Lower volume items
    }
    
    # Portfolio-level MAPE (most fair comparison between holistic vs per-item approaches)
    total_actual = sum(actual_dict.get(item, 0) for item in target_cols)
    total_error = sum(abs(actual_dict.get(item, 0) - pred_dict.get(item, 0)) for item in target_cols)
    portfolio_mape = (total_error / total_actual * 100) if total_actual > 0 else float('inf')
    
    # Business-weighted MAPE (strategic importance weighting)
    # This gives extra weight to strategically important items beyond just volume
    weighted_error = 0
    weighted_actual = 0
    for item in target_cols:
        weight = item_weights.get(item, 0.01)  # Default small weight for unknown items
        actual_val = actual_dict.get(item, 0)
        pred_val = pred_dict.get(item, 0)
        weighted_error += weight * abs(actual_val - pred_val)
        weighted_actual += weight * actual_val
    
    weighted_mape = (weighted_error / weighted_actual * 100) if weighted_actual > 0 else float('inf')
    
    # Worst-case analysis (risk management)
    # Identifies if any single item has catastrophically bad predictions
    item_mapes = []
    for item in target_cols:
        actual_val = actual_dict.get(item, 0)
        pred_val = pred_dict.get(item, 0)
        if actual_val > 0:
            item_mape = abs(actual_val - pred_val) / actual_val * 100
            item_mapes.append(item_mape)
    
    max_item_mape = max(item_mapes) if item_mapes else float('inf')
    
    return {
        'portfolio_mape': portfolio_mape,      # Volume-weighted total accuracy (primary metric)
        'weighted_mape': weighted_mape,        # Strategic importance accuracy (secondary)
        'max_item_mape': max_item_mape,        # Risk management (worst case)
        'total_actual': total_actual,          # Total inventory volume
        'total_error': total_error             # Total absolute error
    }

def get_best_regression_model_info():
    """Get the best performing REGRESSION model using inventory-focused scoring (MAE priority)"""
    try:
        # Try to load regression performance metrics
        import csv
        models_data = []
        
        with open('results/regression/model_comparison_with_cv.csv', 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                model_name = row.get('Model', '')
                if not model_name:  # Skip empty rows
                    continue
                    
                try:
                    mae = float(row.get('MAE', float('inf')))
                    r2 = float(row.get('R2', 0))
                    cv_mae = float(row.get('CV_MAE', mae))  # Cross-validation MAE if available
                except (ValueError, TypeError):
                    continue  # Skip rows with invalid data
                
                models_data.append({
                    'name': model_name,
                    'mae': mae,
                    'r2': r2,
                    'cv_mae': cv_mae
                })
        
        if not models_data:
            raise FileNotFoundError("No valid model data found in results/regression/model_comparison_with_cv.csv")
        
        # Find the model with the best balance of MAE and R¬≤
        valid_models = [model for model in models_data if model['mae'] != float('inf')]
        if not valid_models:
            raise ValueError("No valid models found in the comparison file")
        
        best_model = None
        best_score = -float('inf')
        best_mae = float('inf')
        best_r2 = 0
        
        print(f"üîç Evaluating {len(valid_models)} models for best inventory performance:")
        
        for model in valid_models:
            # Calculate generalization score (penalize if test >> CV performance)
            generalization_penalty = 0
            if model['cv_mae'] > 0:
                mae_diff = model['mae'] - model['cv_mae']
                generalization_penalty = max(0, mae_diff / model['cv_mae'])
            
            # Inventory-focused scoring: prioritize MAE more heavily for business impact
            # Lower MAE is better, higher R¬≤ is better, lower generalization penalty is better
            mae_score = 1 / (1 + model['mae'] / 20)  # Normalize around typical MAE values
            r2_score = model['r2']  # Already 0-1 scale
            generalization_score = 1 / (1 + generalization_penalty)  # Penalize overfitting
            
            # Weighted combination: 60% MAE, 30% R¬≤, 10% generalization (prioritize MAE for inventory)
            composite_score = (mae_score * 0.6 + r2_score * 0.3 + generalization_score * 0.1)
            
            print(f"   {model['name']}: MAE={model['mae']:.2f}, R¬≤={model['r2']:.3f}, Gen={generalization_penalty:.3f}, Score={composite_score:.3f}")
            
            if composite_score > best_score:
                best_score = composite_score
                best_model = model['name']
                best_mae = model['mae']
                best_r2 = model['r2']
        
        if not best_model:
            raise ValueError("Could not determine best model from available data")
        
        print(f"üèÜ Selected: {best_model} (Score: {best_score:.3f})")
        accuracy = best_r2 * 100
        return best_model, f"~{accuracy:.0f}% (¬±{best_mae:.0f} units average error)"
            
    except FileNotFoundError:
        raise FileNotFoundError("Model comparison file not found. Please train regression models first.")
    except Exception as e:
        raise RuntimeError(f"Failed to determine best regression model: {str(e)}")

def get_best_model_info():
    """Get the best performing model info - wrapper for backward compatibility"""
    return get_best_regression_model_info()

def get_model_accuracy_info(model_type='regression'):
    """Get dynamic model accuracy information from saved results"""
    if model_type == 'regression':
        try:
            _, accuracy_info = get_best_regression_model_info()
            return accuracy_info
        except Exception as e:
            raise RuntimeError(f"Cannot get regression model accuracy: {str(e)}")
    elif model_type == 'arima':
        try:
            arima_performance = joblib.load('models/arima/arima_performance.pkl')
            if not arima_performance:
                raise FileNotFoundError("ARIMA performance data is empty")
            avg_mae = np.mean([perf['MAE'] for perf in arima_performance.values()])
            avg_r2 = np.mean([perf['R2'] for perf in arima_performance.values()])
            accuracy = avg_r2 * 100
            return f"~{accuracy:.0f}% (¬±{avg_mae:.0f} units average error)"
        except FileNotFoundError:
            raise FileNotFoundError("ARIMA performance file not found. Please train ARIMA models first.")
        except Exception as e:
            raise RuntimeError(f"Cannot get ARIMA model accuracy: {str(e)}")
    else:
        raise ValueError(f"Unknown model type: {model_type}")

def print_manager_report(forecast_df, target_cols, comparison_results=None, anomaly_info=None):
    """Print a manager-friendly report to console and save to file"""
    
    # Create the report content
    report_lines = []
    report_lines.append("="*60)
    report_lines.append("üçó RESTAURANT INVENTORY FORECAST")
    report_lines.append("="*60)
    
    report_lines.append(f"\nüìÖ Forecast Period: {forecast_df['Date'].iloc[0]} to {forecast_df['Date'].iloc[-1]}")
    
    # Show model information
    model_type_used = 'regression'  # Default
    if 'Model_Type' in forecast_df.columns:
        model_type = forecast_df['Model_Type'].iloc[0]
        model_name = forecast_df['Model_Name'].iloc[0]
        report_lines.append(f"üéØ Model: {model_name} ({model_type})")
        model_type_used = model_type.lower()
    else:
        report_lines.append(f"üéØ Model: Combined Forecast")
    
    # Show comparison results if available
    if comparison_results:
        report_lines.append(f"üìä Model Comparison: {comparison_results}")
        
        # Add portfolio-level insights
        if "Portfolio MAPE" in comparison_results:
            if "Regression wins" in comparison_results:
                report_lines.append(f"üí° Holistic regression captured cross-item relationships effectively")
            elif "ARIMA wins" in comparison_results:
                report_lines.append(f"üí° Per-item ARIMA specialization outperformed holistic approach")
    
    # Show anomaly detection results if available
    if anomaly_info:
        report_lines.append(f"üö® Anomaly Detection: {anomaly_info}")
    
    report_lines.append("\nüìä DAILY RECOMMENDATIONS:")
    report_lines.append("-" * 50)
    
    for _, row in forecast_df.iterrows():
        day_line = f"\nüìÜ {row['Date']} ({row['Day_of_Week']})"
        if row['Is_Weekend']:
            day_line += " üåü WEEKEND"
        report_lines.append(day_line)
        
        report_lines.append("   Recommended Stock Levels:")
        for col in target_cols:
            col_title = col.title()
            stock_val = row[f'{col_title}_Recommended_Stock']
            forecast_val = row[f'{col_title}_Forecast']
            report_lines.append(f"   ‚Ä¢ {col_title:<12}: {stock_val:>3} units (forecast: {forecast_val})")
    
    report_lines.append("\n" + "="*60)
    report_lines.append("üìã WEEKLY TOTALS:")
    report_lines.append("-" * 30)
    
    for col in target_cols:
        col_title = col.title()
        weekly_stock = forecast_df[f'{col_title}_Recommended_Stock'].sum()
        weekly_forecast = forecast_df[f'{col_title}_Forecast'].sum()
        report_lines.append(f"{col_title:<15}: {weekly_stock:>4} units (forecast: {weekly_forecast})")
    
    # Key insights
    report_lines.append("\nüí° KEY INSIGHTS:")
    report_lines.append("-" * 20)
    
    # Weekend analysis
    weekend_count = forecast_df['Is_Weekend'].sum()
    if weekend_count > 0:
        weekend_avg = forecast_df[forecast_df['Is_Weekend']][f'{target_cols[0].title()}_Recommended_Stock'].mean()
        weekday_avg = forecast_df[~forecast_df['Is_Weekend']][f'{target_cols[0].title()}_Recommended_Stock'].mean()
        if weekend_avg > weekday_avg:
            report_lines.append(f"üåü Weekend demand ~{((weekend_avg/weekday_avg-1)*100):.0f}% higher than weekdays")
        else:
            report_lines.append(f"üåü {weekend_count} weekend days in forecast period")
    
    # Peak day - handle NaN values
    total_col = f'{target_cols[0].title()}_Recommended_Stock'  # Use first item as proxy
    if total_col in forecast_df.columns and not forecast_df[total_col].isna().all():
        try:
            peak_idx = forecast_df[total_col].idxmax()
            if not pd.isna(peak_idx):
                peak_day = forecast_df.loc[peak_idx]
                report_lines.append(f"üìà Highest demand day: {peak_day['Day_of_Week']}")
            else:
                report_lines.append(f"üìà Highest demand day: Unable to determine (all values equal)")
        except (KeyError, ValueError):
            report_lines.append(f"üìà Highest demand day: Unable to determine (data issue)")
    else:
        report_lines.append(f"üìà Highest demand day: Unable to determine (no valid data)")
    
    # Get dynamic model accuracy based on actual model used
    try:
        accuracy_info = get_model_accuracy_info(model_type_used)
    except Exception as e:
        # Try to get accuracy from stored model info first
        if hasattr(forecast_df, '_model_accuracy'):
            accuracy_info = forecast_df._model_accuracy
        else:
            # If we can't get the accuracy, show the error instead of lying with fake numbers
            accuracy_info = f"Unable to determine (error: {str(e)[:50]}...)"
    
    report_lines.append("\n‚ö†Ô∏è  NOTES:")
    report_lines.append("‚Ä¢ Stock levels include 20% safety buffer")
    report_lines.append("‚Ä¢ Monitor daily and adjust for special events")
    report_lines.append(f"‚Ä¢ Model accuracy: {accuracy_info}")
    
    # Print to console
    for line in report_lines:
        print(line)
    
    # SAVE THE EXACT SAME DATAFRAME THAT WAS DISPLAYED
    os.makedirs('forecasts/final', exist_ok=True)
    
    # Save to final forecasts directory (the correct files)
    with open('forecasts/final/RESTAURANT_TOOL_FORECAST.txt', 'w') as f:
        for line in report_lines:
            f.write(line + '\n')
    
    # Save the exact DataFrame that was displayed to CSV
    forecast_df.to_csv('forecasts/final/RESTAURANT_TOOL_FORECAST.csv', index=False)
    
    # Save debug information to verify data integrity
    with open('forecasts/final/FORECAST_DEBUG_INFO.txt', 'w') as f:
        f.write("FORECAST DATA INTEGRITY CHECK\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"DataFrame shape: {forecast_df.shape}\n")
        f.write(f"DataFrame columns: {list(forecast_df.columns)}\n\n")
        
        f.write("SAMPLE DATA (first 3 rows):\n")
        f.write("-" * 30 + "\n")
        for i, (_, row) in enumerate(forecast_df.head(3).iterrows()):
            f.write(f"Row {i+1}: {row['Date']} ({row['Day_of_Week']})\n")
            for col in target_cols:
                col_title = col.title()
                if f'{col_title}_Forecast' in row:
                    f.write(f"  {col_title}: {row[f'{col_title}_Forecast']} forecast, {row[f'{col_title}_Recommended_Stock']} stock\n")
            f.write("\n")
        
        f.write("WEEKLY TOTALS VERIFICATION:\n")
        f.write("-" * 30 + "\n")
        for col in target_cols:
            col_title = col.title()
            if f'{col_title}_Forecast' in forecast_df.columns:
                weekly_forecast = forecast_df[f'{col_title}_Forecast'].sum()
                weekly_stock = forecast_df[f'{col_title}_Recommended_Stock'].sum()
                f.write(f"{col_title}: {weekly_forecast} forecast, {weekly_stock} stock\n")
    
    print(f"\nüíæ Final forecast saved to: forecasts/final/RESTAURANT_TOOL_FORECAST.txt")
    print(f"üíæ Final CSV saved to: forecasts/final/RESTAURANT_TOOL_FORECAST.csv")
    print(f"üíæ Debug info saved to: forecasts/final/FORECAST_DEBUG_INFO.txt")

def save_forecast_csv(forecast_df, filename="forecasts/next_week_forecast.csv"):
    """Save forecast to CSV file - EXACT same DataFrame that was displayed"""
    os.makedirs('forecasts', exist_ok=True)
    os.makedirs('forecasts/final', exist_ok=True)
    
    # Save the EXACT DataFrame as-is (no modifications)
    forecast_df.to_csv(filename, index=False)
    print(f"\nüíæ Forecast CSV saved to: {filename}")
    
    # Debug: Print first few rows to verify data integrity
    print(f"üìä CSV Data Preview (first 2 rows):")
    for i, (_, row) in enumerate(forecast_df.head(2).iterrows()):
        print(f"   Row {i+1}: {row['Date']} - Wings: {row.get('Wings_Forecast', 'N/A')}, Tenders: {row.get('Tenders_Forecast', 'N/A')}")


def main():
    parser = argparse.ArgumentParser(description='Generate restaurant inventory forecast')
    parser.add_argument('--dataset', type=str, required=False, 
                       help='Path to the dataset CSV file (required unless using --predict with no historical data needed)')
    parser.add_argument('--days', type=int, default=7, help='Number of days to forecast (default: 7)')
    parser.add_argument('--save-csv', action='store_true', help='Save results to CSV file')
    parser.add_argument('--model', choices=['regression', 'arima', 'both'], default='both', 
                       help='Which model type to use (default: both)')
    parser.add_argument('--predict', action='store_true', help='Use pre-trained models (skip training)')
    parser.add_argument('--anomaly-detection', action='store_true', help='Run anomaly detection on historical data')
    parser.add_argument('--train-anomaly', action='store_true', help='Train anomaly detection model')
    parser.add_argument('--anomaly-threshold', choices=['conservative', 'balanced', 'sensitive'], 
                       default='balanced', help='Anomaly detection sensitivity (default: balanced)')
    args = parser.parse_args()
    
    print("üçó Restaurant Inventory Forecasting Tool")
    print("=" * 50)
    
    # Determine if we need historical data
    need_historical_data = not args.predict or args.model in ['regression', 'both']
    
    # Load recent data if needed
    recent_df = None
    target_cols = None
    
    if need_historical_data or not args.predict:
        if not args.dataset:
            print("‚ùå Error: Dataset path is required when training models or generating features from historical data")
            print("Use --dataset /path/to/your/dataset.csv")
            print("Example: uv run restaurant_forecast_tool.py --dataset data/inventory_delivery_forecast_data.csv")
            return
        
        recent_df = get_recent_data(args.dataset)
        if recent_df is None:
            return
        
        # Get target columns dynamically from the data
        exclude_cols = ['delivery_date', 'day_of_week', 'month', 'day_of_month', 'is_weekend', 'days_since_start']
        target_cols = [col for col in recent_df.columns if col not in exclude_cols and not any(suffix in col for suffix in ['_lag', '_roll', '_ratio', '_total'])]
        print(f"üìä Detected target columns: {target_cols}")
    
    # For predict-only mode with ARIMA, try to get target columns from saved models
    if args.predict and args.model == 'arima' and target_cols is None:
        try:
            arima_performance = joblib.load('models/arima/arima_performance.pkl')
            target_cols = list(arima_performance.keys())
            print(f"üìä Target columns from saved ARIMA models: {target_cols}")
        except:
            print("‚ùå Error: Cannot determine target columns. Please provide dataset or ensure ARIMA models are trained.")
            return
    
    # Generate forecast features if we have historical data
    forecast_features = None
    if recent_df is not None:
        print(f"üîÆ Generating {args.days}-day forecast features from historical data...")
        forecast_features = generate_forecast_features(recent_df, args.days)
    
    regression_forecast = None
    arima_forecast = None
    comparison_results = None
    anomaly_info = None
    
    # Handle anomaly detection - auto-train if model doesn't exist
    if args.dataset and AUTOENCODER_AVAILABLE:
        # Check if anomaly model exists, if not train it automatically
        if not check_anomaly_model_exists():
            print("üîß Anomaly detection model not found. Training automatically...")
            print("   This is a one-time setup that may take a few minutes...")
            train_anomaly_detector(args.dataset, n_trials=25)  # Reduced trials for faster initial setup
            print("‚úÖ Anomaly detection model trained successfully!")
        else:
            print("‚úÖ Found existing anomaly detection model - ready for monitoring")
        
        # Run anomaly detection if requested
        if args.anomaly_detection:
            print("üîç Running anomaly detection on historical data...")
            anomaly_results = run_anomaly_detection(args.dataset, args.anomaly_threshold)
            if anomaly_results is not None:
                total_anomalies = anomaly_results['is_anomaly'].sum()
                anomaly_percentage = 100 * total_anomalies / len(anomaly_results)
                anomaly_info = f"{total_anomalies} anomalies detected ({anomaly_percentage:.1f}%)"
                
                if total_anomalies > 0:
                    recent_anomalies = anomaly_results[anomaly_results['is_anomaly']].tail(5)
                    print(f"\nüö® Recent anomalous delivery windows:")
                    for _, row in recent_anomalies.iterrows():
                        print(f"   {row['delivery_date'].strftime('%Y-%m-%d')}: Total inventory {row['total_inventory']:.0f}")
    elif args.dataset and not AUTOENCODER_AVAILABLE and (args.anomaly_detection or args.train_anomaly):
        print("‚ö†Ô∏è  Autoencoder functionality not available due to TensorFlow/NumPy compatibility issues.")
        print("   To fix: downgrade NumPy with 'pip install numpy<2' or upgrade TensorFlow")
    
    # Handle explicit anomaly model training if requested
    if args.train_anomaly:
        if not args.dataset:
            print("‚ùå Error: Dataset path is required for anomaly model training")
            return
        
        if not AUTOENCODER_AVAILABLE:
            print("‚ö†Ô∏è  Autoencoder functionality not available due to TensorFlow/NumPy compatibility issues.")
            print("   To fix: downgrade NumPy with 'pip install numpy<2' or upgrade TensorFlow")
            return
        
        print("üîß Re-training anomaly detection model with full optimization...")
        train_anomaly_detector(args.dataset, n_trials=75)  # Full trials for explicit training
        print("‚úÖ Anomaly detection model training completed!")
    
    # Handle regression models
    if args.model in ['regression', 'both']:
        if args.predict:
            # Use pre-trained regression models
            if forecast_features is None:
                print("‚ùå Error: Historical data needed for regression predictions to generate features")
                return
            regression_models, scaler, selector_info = load_regression_models()
            if regression_models is not None:
                regression_forecast = make_regression_predictions(regression_models, scaler, selector_info, forecast_features, target_cols)
                print("‚úÖ Regression forecast generated using pre-trained models")
        else:
            # Train new regression models
            print("üîÑ Training regression models...")
            try:
                # Set matplotlib backend before training
                os.environ['MPLBACKEND'] = 'Agg'
                
                # Import and run regression training directly
                from inventory_forecasting_regression import main as train_regression
                best_model_info = train_regression(args.dataset)
                
                # Load the newly trained models
                regression_models, scaler, selector_info = load_regression_models()
                if regression_models is not None and forecast_features is not None:
                    # Use the best model name from training if available
                    best_model_name = best_model_info.get('best_model_name') if best_model_info else None
                    regression_forecast = make_regression_predictions(regression_models, scaler, selector_info, forecast_features, target_cols, best_model_name, best_model_info)
                    print("‚úÖ Regression forecast generated with newly trained models")
                else:
                    print("‚ùå Failed to load regression models or generate features")
            except Exception as e:
                print(f"‚ùå Regression training failed: {str(e)}")
                import traceback
                traceback.print_exc()
    
    # Handle ARIMA models
    if args.model in ['arima', 'both'] and ARIMA_AVAILABLE:
        if args.predict:
            # Use pre-trained ARIMA models
            arima_models, arima_performance, model_metadata, arima_target_cols = load_arima_models()
            if arima_models is not None:
                # For ARIMA, we can generate simple forecast features without full historical data
                if forecast_features is None:
                    # Create minimal forecast features for ARIMA (just calendar features)
                    from datetime import datetime, timedelta
                    last_date = datetime.now()
                    future_dates = [last_date + timedelta(days=i) for i in range(1, args.days + 1)]
                    
                    forecast_features = pd.DataFrame()
                    forecast_features['forecast_date'] = future_dates
                    forecast_features['day_of_week'] = [d.weekday() for d in future_dates]
                    forecast_features['is_weekend'] = [int(d.weekday() >= 5) for d in future_dates]
                    forecast_features['month'] = [d.month for d in future_dates]
                    
                    print("‚ö†Ô∏è  Using current date for ARIMA forecast (no historical data provided)")
                
                arima_forecast = make_arima_predictions(arima_models, model_metadata, forecast_features, arima_target_cols)
                print("‚úÖ ARIMA forecast generated using pre-trained models")
        else:
            # Train new ARIMA models
            print("üîÑ Training ARIMA models...")
            try:
                # Pass dataset path to training function
                import sys
                sys.argv = ['arima_forecasting.py', '--dataset', args.dataset]
                arima_results = run_arima_pipeline(args.dataset)
                if arima_results:
                    arima_models, arima_performance, model_metadata, arima_target_cols = load_arima_models()
                    if arima_models is not None:
                        arima_forecast = make_arima_predictions(arima_models, model_metadata, forecast_features, arima_target_cols)
                        print("‚úÖ ARIMA forecast generated with newly trained models")
            except Exception as e:
                print(f"‚ùå ARIMA training failed: {str(e)}")
    elif args.model in ['arima', 'both'] and not ARIMA_AVAILABLE:
        print("‚ö†Ô∏è  ARIMA models not available. Install statsmodels: pip install statsmodels")
    
    # Compare models and select best forecast using portfolio-level metrics
    final_forecast = None
    comparison_results = None
    
    if regression_forecast is not None and arima_forecast is not None:
        print(f"\nüîç PORTFOLIO-LEVEL MODEL COMPARISON:")
        print("=" * 50)
        
        # Create prediction dictionaries for portfolio comparison
        # We need to simulate actual vs predicted for fair comparison
        # Since we don't have actual test data here, we'll use the training performance metrics
        
        # Get regression performance from training results
        reg_portfolio_mape = None
        reg_mae = None
        if 'best_model_info' in locals() and best_model_info and 'best_model_metrics' in best_model_info:
            reg_mae = best_model_info['best_model_metrics']['MAE']
            if 'MAPE' in best_model_info['best_model_metrics']:
                reg_portfolio_mape = best_model_info['best_model_metrics']['MAPE']
            print(f"üìä Regression (Holistic): MAE={reg_mae:.2f}, MAPE={reg_portfolio_mape:.1f}%" if reg_portfolio_mape else f"üìä Regression (Holistic): MAE={reg_mae:.2f}")
        
        # Get ARIMA performance from training results
        arima_portfolio_mape = None
        arima_mae = None
        if 'arima_results' in locals() and arima_results:
            arima_mae = arima_results.get('overall_mae')
            arima_portfolio_mape = arima_results.get('overall_mape')
            print(f"üìä ARIMA (Per-Item): MAE={arima_mae:.2f}, MAPE={arima_portfolio_mape:.1f}%" if arima_portfolio_mape else f"üìä ARIMA (Per-Item): MAE={arima_mae:.2f}")
        
        # Model selection logic with portfolio focus
        winner_selected = False
        
        # Primary comparison: Portfolio MAPE (most fair for holistic vs per-item)
        if reg_portfolio_mape is not None and arima_portfolio_mape is not None:
            print(f"\nüéØ Portfolio MAPE Comparison:")
            print(f"   Regression (Holistic): {reg_portfolio_mape:.1f}%")
            print(f"   ARIMA (Per-Item Avg): {arima_portfolio_mape:.1f}%")
            
            if reg_portfolio_mape <= arima_portfolio_mape:
                final_forecast = regression_forecast
                improvement = ((arima_portfolio_mape - reg_portfolio_mape) / arima_portfolio_mape * 100)
                comparison_results = f"Regression wins (Portfolio MAPE: {reg_portfolio_mape:.1f}% vs {arima_portfolio_mape:.1f}%, {improvement:.1f}% better)"
                print(f"üèÜ Winner: Regression - {improvement:.1f}% better portfolio accuracy")
                print(f"üí° Holistic approach captures cross-item relationships better")
            else:
                final_forecast = arima_forecast
                improvement = ((reg_portfolio_mape - arima_portfolio_mape) / reg_portfolio_mape * 100)
                comparison_results = f"ARIMA wins (Portfolio MAPE: {arima_portfolio_mape:.1f}% vs {reg_portfolio_mape:.1f}%, {improvement:.1f}% better)"
                print(f"üèÜ Winner: ARIMA - {improvement:.1f}% better portfolio accuracy")
                print(f"üí° Per-item specialization outperforms holistic approach")
            winner_selected = True
        
        # Fallback comparison: MAE
        elif reg_mae is not None and arima_mae is not None:
            print(f"\n‚ö†Ô∏è  Using MAE comparison (Portfolio MAPE not available)")
            print(f"   Regression MAE: {reg_mae:.2f}")
            print(f"   ARIMA MAE: {arima_mae:.2f}")
            
            if reg_mae <= arima_mae:
                final_forecast = regression_forecast
                improvement = ((arima_mae - reg_mae) / arima_mae * 100)
                comparison_results = f"Regression wins (MAE: {reg_mae:.2f} vs {arima_mae:.2f}, {improvement:.1f}% better)"
                print(f"üèÜ Winner: Regression ({improvement:.1f}% better)")
            else:
                final_forecast = arima_forecast
                improvement = ((reg_mae - arima_mae) / reg_mae * 100)
                comparison_results = f"ARIMA wins (MAE: {arima_mae:.2f} vs {reg_mae:.2f}, {improvement:.1f}% better)"
                print(f"üèÜ Winner: ARIMA ({improvement:.1f}% better)")
            winner_selected = True
        
        # Final fallback
        if not winner_selected:
            final_forecast = regression_forecast
            comparison_results = "Using Regression (performance comparison unavailable)"
            print("‚ö†Ô∏è  Using Regression model (performance metrics not available)")
        
        # Additional insights
        print(f"\nüí° Model Comparison Insights:")
        if reg_portfolio_mape and arima_portfolio_mape:
            if abs(reg_portfolio_mape - arima_portfolio_mape) < 2.0:
                print(f"   üìä Models perform similarly (difference < 2%)")
                print(f"   üîÑ Consider ensemble approach for robustness")
            elif reg_portfolio_mape < arima_portfolio_mape:
                print(f"   üîó Holistic regression captures item relationships well")
                print(f"   üìà Cross-correlations (wings‚Üídips) provide forecasting advantage")
            else:
                print(f"   üéØ Per-item ARIMA specialization wins")
                print(f"   üìä Individual item patterns too complex for holistic approach")
        
    elif regression_forecast is not None:
        final_forecast = regression_forecast
        comparison_results = "Regression only"
        print("üìä Using Regression model (only available model)")
    elif arima_forecast is not None:
        final_forecast = arima_forecast
        comparison_results = "ARIMA only"
        print("üìä Using ARIMA model (only available model)")
    else:
        print("‚ùå No forecasts could be generated!")
        return
    
    # Display results
    if target_cols is None and final_forecast is not None:
        # Extract target columns from forecast dataframe
        forecast_cols = [col for col in final_forecast.columns if col.endswith('_Forecast')]
        target_cols = [col.replace('_Forecast', '').lower() for col in forecast_cols]
    
    print_manager_report(final_forecast, target_cols, comparison_results, anomaly_info)
    
    # ALWAYS save the exact forecast DataFrame that was displayed
    # This ensures CLI output matches saved files
    print(f"\nüíæ SAVING FORECAST DATA:")
    print(f"üìä Final forecast DataFrame shape: {final_forecast.shape}")
    print(f"üìä Final forecast columns: {list(final_forecast.columns)}")
    
    # Save to CSV if requested
    if args.save_csv:
        save_forecast_csv(final_forecast)
        save_forecast_csv(final_forecast, "forecasts/final/RESTAURANT_TOOL_FORECAST.csv")
        
        # Also save individual model forecasts if both were generated
        if regression_forecast is not None and arima_forecast is not None:
            save_forecast_csv(regression_forecast, "forecasts/regression_forecast.csv")
            save_forecast_csv(arima_forecast, "forecasts/arima_forecast.csv")
    else:
        # Always save the final forecast even if not explicitly requested
        save_forecast_csv(final_forecast, "forecasts/final/RESTAURANT_TOOL_FORECAST.csv")
    
    print(f"\n‚úÖ Forecast complete! Plan your inventory accordingly.")

if __name__ == "__main__":
    main()
